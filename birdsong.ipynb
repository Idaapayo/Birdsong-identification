{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ef2b191dc078>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import kaggle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maudioread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# import kaggle\n",
    "\n",
    "import cv2 as cv\n",
    "import audioread\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# from colored import fg, bg, attr\n",
    "\n",
    "import librosa\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment as AS\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.optim import Adam\n",
    "from torch import FloatTensor, LongTensor, DoubleTensor\n",
    "# from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import tensorflow as tf\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences as pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for utilities\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_logger(out_file=None):\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "#     logger.addHandler(handler)\n",
    "    \n",
    "    if out_file is not None:\n",
    "        fh = logging.FileHandler(out_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "    logger.info(\"logger set up\")\n",
    "    return logger\n",
    "\n",
    "@contextmanager #to ensure output of time is string\n",
    "def timer(name: str, logger: Optional[logging.Logger] = None):\n",
    "    t0 = time.time()\n",
    "    msg = f\"[{name}] start\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    yield\n",
    "    \n",
    "    msg =  f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/Asus/jypNotebooks/birdsong classification/train.csv')\n",
    "test = pd.read_csv('C:/Users/Asus/jypNotebooks/birdsong classification/test.csv')\n",
    "audio_path = \"C:/Users/Asus/jypNotebooks/birdsong classification/train_audio\"\n",
    "TEST = Path(\"C:/Users/Asus/jypNotebooks/birdsong classification/test_audio\").exists()\n",
    "BASE_DIR = (\"C:/Users/Asus/jypNotebooks/birdsong classification\")\n",
    "if TEST:\n",
    "    DATA_DIR = Path(\"C:/Users/Asus/jypNotebooks/birdsong classification\")\n",
    "else:\n",
    "    # dataset created by @shonenkov, thanks!\n",
    "    DATA_DIR = Path(\"C:/Users/Asus/jypNotebooks/birdsong classification/testcheck.csv\")\n",
    "    \n",
    "test_audio = DATA_DIR / \"test_audio\"\n",
    "\n",
    "# train_extend = pd.read_csv(\"../input/xeno-canto-bird-recordings-extended-a-m/train_extended.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 44100\n",
    "EPOCHS = 20\n",
    "MAXLEN = 1000000\n",
    "CHUNK_SIZE = 1000000\n",
    "CHUNKS = 3\n",
    "N_MELS = 256 #no of melspectrogram features per time step\n",
    "MEL_LEN = 1954 #total no of time steps in each melspectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different kind of one hot encoding\n",
    "\n",
    "keys = set(train[\"ebird_code\"])\n",
    "values = np.arange(0, len(keys))\n",
    "code_dict = dict(zip(sorted(keys), values))\n",
    "print(code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INV_BIRD_CODE = {v: k for k, v in code_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirname = train[\"ebird_code\"]\n",
    "# filename = train[\"filename\"]\n",
    "# base_path = \"../input/birdsong-recognition/train_audio\"\n",
    "\n",
    "# for dirname,filename in zip(dirname, filename):\n",
    "#     path = base_path + '/'+ dirname + '/'+ filename\n",
    "#     size_file = os.path.getsize(path)\n",
    "#     if size_file == 0:\n",
    "#         print('Empty label file:', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''spliting into train and cross-val 80% train 20% val'''\n",
    "# train = shuffle(train)\n",
    "split = int(0.8*len(train))\n",
    "train = train.reset_index(drop = True)\n",
    "val = train[split:].reset_index(drop = True)\n",
    "train = train[:split].reset_index(drop = True)\n",
    "# train, val = train_test_split(train, test_size = 0.2)\n",
    "print(len(train))\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n-mels - no of mel bands to generate\n",
    "fmin - min frequency\n",
    "fmax - max frequency\n",
    "'''\n",
    "\n",
    "melspec_params = {\"n_mels\": 128, \"fmin\":20, \"fmax\":1600}\n",
    "\n",
    "'''\n",
    "dict with params for model \n",
    "\n",
    "'''\n",
    "model_config = {\"base_model_name\": \"resnet50\", \"pretrained\": False, \"num_classes\": 264 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return np.float(x)/2**15\n",
    "\n",
    "def read(file, norm = False):\n",
    "    try : a = AS.from_mp3(file)\n",
    "    except: return np.zeros(MAXLEN)\n",
    "    \n",
    "    y = np.array(a.get_array_of_samples())\n",
    "    if a.channels == 2: y = y.reshape((-1, 2))\n",
    "        \n",
    "    if norm: return a.frame_rate, normalize(y)\n",
    "    if not norm: return a.frame_rate, np.float32(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(length):\n",
    "    '''get the maximum length of a signal'''\n",
    "    if length > MAXLEN : return MAXLEN\n",
    "    if length <= MAXLEN : return int(length * 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(length):\n",
    "    '''select start and end index of a given audio chunk'''\n",
    "    length = get_len(length)\n",
    "    idx = np.random.randint(length + 1)\n",
    "    chunk_range = idx , idx + CHUNK_SIZE\n",
    "    chunk_idx = max([0, chunk_range[0]])\n",
    "    chunk_idx = min([chunk_range[1], 0])\n",
    "    return (chunk_idx, chunk_idx + CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk(data, length):\n",
    "    \"\"\"takes index from chunk data and outputs a given chunk\"\"\"\n",
    "    index = get_idx(length)\n",
    "    return data[index[0]:index[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal(data):\n",
    "    length = max(data.shape)\n",
    "    data = data.flatten().reshape(1,-1)\n",
    "    data = np.float32(pad(data, maxlen = MAXLEN).reshape(-1))\n",
    "    return [get_chunk(data, length) for _ in range(CHUNKS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(data):\n",
    "    return [FloatTensor(point) for point in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #if submission is succesful file will be overwritten\n",
    "# sub = pd.read_csv(\"../input/birdsong-recognition/sample_submission.csv\")\n",
    "# sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a model\n",
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    Define a class of neural networks and override the feed foward function \n",
    "    '''\n",
    "    def __init__(self, base_model_name: str,  pretrained = False, num_classes=264):\n",
    "        super().__init__()\n",
    "        base_model = models.__getattribute__(base_model_name)(pretrained = pretrained)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "#         layers = []\n",
    "\n",
    "        layers.append(nn.AdaptiveMaxPool2d(1))\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        in_features = base_model.fc.in_features\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(in_features, 1024), nn.ReLU(), nn.Dropout(p=0.2), nn.ReLU(), nn.Dropout(p=0.2), nn.Linear(1024, num_classes))\n",
    "                                        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.encoder(x).view(batch_size, -1)#-1 is used when you are sure of the no of rows/cols but not sure of the other\n",
    "        x = self.classifier(x)\n",
    "        multiclass_prob = F.softmax(x, dim=1) #helps assign decimal values to a multi-class problem\n",
    "        multilabel_prob = F.sigmoid(x)\n",
    "        return {\"logits\": x, \"multiclass_prob\": multiclass_prob, \"multilabel_prob\": multilabel_prob}\n",
    "            \n",
    "                                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_to_color(X: np.ndarray, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "    \n",
    "    #Standardize\n",
    "    mean = mean or X.mean()\n",
    "    X = X - mean\n",
    "    std = std or X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    \n",
    "    if (_max - _min) > eps:\n",
    "        #Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min)/ (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        #return only zeros\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "     y, sr = librosa.load(path, sr = SR)\n",
    "#     except: \n",
    "#         print(\"returning zeros\")\n",
    "#         return np.zeros(MAXLEN)\n",
    "    \n",
    "     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_melspectogram(audio_ts):\n",
    "    melspec = librosa.feature.melspectrogram(audio_ts, sr=SR, **melspec_params)\n",
    "    melspc = librosa.power_to_db(melspec).astype(np.float32)\n",
    "    \n",
    "    return melspc\n",
    "\n",
    "def get_melsp_img(data):\n",
    "    data = get_signal(data)\n",
    "    mel = np.stack([to_melspectogram(point) for point in data])\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining bird dataset for trainning audio files for a specific class\n",
    "\n",
    "class BirdDataset(data.Dataset):\n",
    "    '''class defining the birds dataset to be fed to a model to identify the \n",
    "       types of birds'''\n",
    "    def __init__(self, df, path, img_size = 255, transform = None):\n",
    "        self.code_dict = code_dict\n",
    "        self.classes = len(code_dict)\n",
    "        self.df , self.path = df, path\n",
    "        self.dataset_length = len(df)\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file_name = self.df.filename[i]\n",
    "        ebird_code = self.df.ebird_code[i]\n",
    "        num_code = self.code_dict[ebird_code]\n",
    "        \n",
    "        default_signal = np.random.random(MAXLEN)*1000\n",
    "        default_values = SR, np.int32(np.round(default_signal))\n",
    "        \n",
    "        exists = os.path.exists(self.path + '/' + ebird_code + '/' + file_name )\n",
    "#         print(exists)\n",
    "        clip = load_audio(self.path + '/' + ebird_code + '/' + file_name)\n",
    "#         print(clip)\n",
    "        y_batch = get_chunk(clip, 10000)\n",
    "        print(y_batch)\n",
    "        \n",
    "#         len_clip = len(clip)\n",
    "#         start = 0\n",
    "#         end = SR * 5\n",
    "#         images = []\n",
    "        \n",
    "#         while len_clip > start:\n",
    "#             y_batch = clip[start:end]\n",
    "#             if len(y_batch) != (SR * 5):\n",
    "#                 break\n",
    "#             start = end\n",
    "#             end = end + SR * 5\n",
    "            \n",
    "#         if y_batch.any():\n",
    "#             if os.path.exists(\"C:/Users/Asus/jypNotebooks/birdsong classification/train_audio/melspecs.npy\"):\n",
    "#                 resized_img = np.load(\"C:/Users/Asus/jypNotebooks/birdsong classification/train_audio/melspecs.npy\")\n",
    "#             else:\n",
    "        melspec = to_melspectogram(y_batch)\n",
    "        image = mono_to_color(melspec)\n",
    "        #resizing the images\n",
    "        width = 255\n",
    "        height = image.shape[0]\n",
    "        dim = (width, height)\n",
    "        resized_img = cv.resize(image, dim)\n",
    "        resized_img = np.moveaxis(resized_img, 2, 0)\n",
    "        resized_shape = resized_img.shape\n",
    "        np.save(BASE_DIR + \"/\" + \"melspecs.npy\", resized_img)\n",
    "        code = to_categorical([num_code], num_classes = self.classes)\n",
    "\n",
    "        return to_tensor([resized_img, code])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_dfs = BirdDataset(train , \"C:/Users/Asus/jypNotebooks/birdsong classification/train_audio\", transform = data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_df = data.DataLoader(bird_dfs, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (x, y) in enumerate(bird_df):\n",
    "    print(x.shape)\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_train_df = BirdDataset(train, \"C:/Users/Asus/jypNotebooks/birdsong classification/train_audio\", transform = data_transform)\n",
    "bird_val_df = BirdDataset(val, \"C:/Users/Asus/jypNotebooks/birdsong classification/train_audio\", transform = data_transform)\n",
    "\n",
    "bird_loader_train = tqdm(data.DataLoader(bird_train_df, batch_size = 16, num_workers = 4 ))\n",
    "bird_loader_val = tqdm(data.DataLoader(bird_val_df, batch_size = 16, num_workers = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bird_loader_train))\n",
    "print(len(bird_loader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''init the model'''\n",
    "model1 = ResNet(**model_config).to(device)\n",
    "\n",
    "'''init optimizer'''\n",
    "optimizer = Adam([{'params': model1.encoder.parameters(), 'lr': 0.001},\n",
    "                  {'params': model1.classifier.parameters(), 'lr': 0.001}])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''define cross entropy loss and accuracy'''\n",
    "def cel(y_true, y_pred):\n",
    "    y_true = torch.argmax(y_true ,axis = -1).squeeze()\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    criterion = loss(y_pred, y_true)\n",
    "    return criterion\n",
    "\n",
    "def accuracy(y_true, y_soft_pred):\n",
    "    y_true = torch.argmax(y_true, axis = -1).squeeze()\n",
    "    y_soft_pred = torch.argmax(y_soft_pred, axis = -1).squeeze()\n",
    "    acc = (y_true == y_soft_pred).float().sum()/len(y_true)\n",
    "    acc = torch.round(acc)*100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffle_idx(tensor):\n",
    "    return shuffle(np.arange(len(tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric(data, batch, epoch, start, end, metric, typ):\n",
    "    t = typ, metric, \"%s\", data, \"%s\"\n",
    "    if typ == \"train\": pre = \"BATCH \" + str(batch-1) + \" \"\n",
    "    if typ == \"val\" : pre = \"\\nEPOCH \" +str(epoch+1) + \" \" \n",
    "    time = np.round(end - start, 1); time = \"Time: {} s\".format(time)\n",
    "#     fonts = [(fg(211), attr('reset')), (fg(212), attr('reset')), (fg(213), attr('reset'))]\n",
    "    print(pre  + \"{} {} : {}{}{}\".format(*t) + \" \" + time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open (\"C:/Users/Asus/jypNotebooks/birdsong classification/weightsmd1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = (3, 128, 255)\n",
    "PATH = \"C:/Users/Asus/jypNotebooks/birdsong classification/weightsmd1.pth\"    \n",
    "    \n",
    "          \n",
    "'''training the model'''\n",
    "start = time.time()\n",
    "# print(f\"start time is.{start}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    \n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"state_dict\": model1.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(state, PATH )\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model1.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    model1.train()\n",
    "    \n",
    "    batch = 1\n",
    "    \n",
    "    \n",
    "    for train_x, train_y in bird_loader_train:\n",
    "        \n",
    "        idx = get_shuffle_idx(train_x)\n",
    "        train_x = train_x[idx].to(device)\n",
    "        train_y = train_y[idx].to(device)\n",
    "        \n",
    "        train_preds = model1.forward(train_x)\n",
    "        \n",
    "        train_outputs = train_preds[\"logits\"]\n",
    "        train_outputs = torch.softmax(train_outputs, dim = 1)\n",
    "        train_loss = cel(train_y, train_outputs)\n",
    "        train_softmax = train_preds[\"multilabel_prob\"]    \n",
    "        train_acc = accuracy(train_y, train_softmax)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        end = time.time()\n",
    "        batch = batch + 1\n",
    "        \n",
    "        is_print = batch % 100 == 1\n",
    "        if is_print: print_metric(train_acc, batch, 0, start, end, \"Acc\",\"train\")\n",
    "            \n",
    "    valid_epoch_loss = 0\n",
    "    valid_epoch_acc = 0\n",
    "    \n",
    "    \n",
    "    model1.eval()   \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for valid_x, valid_y in bird_loader_val:\n",
    "            idx = get_shuffle_idx(valid_x)\n",
    "            \n",
    "            valid_x = valid_x[idx].to(device)\n",
    "            valid_y = valid_y[idx].to(device)\n",
    "            \n",
    "            valid_preds = model1.foward(valid_x)\n",
    "            \n",
    "            valid_softmax = vaid_preds['multilabel_prob']\n",
    "            valid_loss = cel(valid_y, valid_softmax)\n",
    "            valid_acc = accuracy(valid_y, valid_softmax)\n",
    "            \n",
    "            valid_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "            \n",
    "            \n",
    "    end = time.time()\n",
    "    \n",
    "#     print_metric(acc, 0, epoch, start, end, \"Acc\", \"Val\"); print(\"\")\n",
    "    \n",
    "    print('ENDING TRAINING...')\n",
    "loss_stats['train'].append(train_epoch_loss/len(bird_loader_train))\n",
    "loss_stats['val'].append(val_epoch_loss/len(bird_loader_val))\n",
    "\n",
    "acc_stats['train'].append(train_epoch_acc/len(bird_loader_train))\n",
    "acc_stats['val'].append(val_epoch_acc/len(bird_loader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data.Dataset):\n",
    "    '''\n",
    "    Class that defines the dataset that will be fed into the model\n",
    "    '''\n",
    "    def __init__(self, dfs: pd.DataFrame, clip: np.ndarray, img_size =224, melspect_params={}):\n",
    "        self.dfs = dfs\n",
    "        self.clip = clip\n",
    "        self.img_size = img_size\n",
    "        self.melspect_params = melspect_params\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dfs)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        sr = 32000\n",
    "        sample = self.dfs.loc[idx, :]\n",
    "        site = sample['site']\n",
    "        row_id = sample['row_id']\n",
    "        if site ==\"site_3\":\n",
    "            y = self.clip.astype(np.float32)\n",
    "            len_y = len(y)\n",
    "            start = 0\n",
    "            end = sr * 5\n",
    "            images = []\n",
    "            while len_y > start:\n",
    "                y_batch = y[start:end].astype(np.float32)\n",
    "                if len(y_batch) != (sr * 5):\n",
    "                    break\n",
    "                start = end\n",
    "                end = end + sr * 5\n",
    "                \n",
    "                melspec = librosa.feature.melspectrogram(y_batch, sr=sr, **melspec_params)\n",
    "                melspc = librosa.power_to_db(melspec).astype(np.float32)\n",
    "                \n",
    "                image = mono_to_color(melspec)\n",
    "                height, width, _ = image.shape\n",
    "                image = cv.resize(image, (int(width * self.img_size/height), self.img_size))\n",
    "                image = np.moveaxis(image, 2, 0)\n",
    "                image = (image/255.0).astype(np.float32)\n",
    "                images.append(image)\n",
    "                \n",
    "            images = np.asarray(images)\n",
    "            return images, row_id, site\n",
    "        else:\n",
    "            end_seconds = int(sample['seconds'])\n",
    "            start_seconds = int(end_seconds - 5)\n",
    "            \n",
    "            start_index = sr * start_seconds\n",
    "            end_index = sr * end_seconds\n",
    "            \n",
    "            y = self.clip[start_index:end_index].astype(np.float32)\n",
    "            \n",
    "            melspec = librosa.feature.melspectrogram(y, sr=sr, **melspec_params)\n",
    "            melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "            \n",
    "            image = mono_to_color(melspec)\n",
    "            height, width, _ = image.shape\n",
    "            image = image = cv.resize(image, (int(width * self.img_size/height), self.img_size))\n",
    "            image = np.moveaxis(image, 2, 0)\n",
    "            image = (image/255.0).astype(np.float32)\n",
    "            \n",
    "            return image, row_id, site\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config: dict):\n",
    "    model = ResNet(**model_config)\n",
    "#     checkpoint = torch.load(weights_path)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_for_clip(test_df:pd.DataFrame, clip:np.ndarray, model:ResNet, mel_params:dict, threshold = 0.5):\n",
    "    \n",
    "    dataset = TestDataset(test, clip=clip, img_size=224, melspect_params = mel_params)\n",
    "    loader = data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    prediction_dict ={}\n",
    "    \n",
    "    for image, row_id, site in progress_bar(loader):\n",
    "        site = site[0]\n",
    "        row_id = row_id[0]\n",
    "        \n",
    "        if site in {\"site_1\", \"site_2\"}:\n",
    "            image = image.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                prediction = model(image)\n",
    "                #no more tracking operations and pick a suitable dimention\n",
    "                proba = prediction[\"multilabel_prob\"].detach().cpu().numpy().reshape(-1)\n",
    "                \n",
    "                events = proba >= threshold\n",
    "                labels = np.argwhere(events).reshape(-1).tolist()\n",
    "                \n",
    "        else:\n",
    "            #avoiding prediction on large batch\n",
    "            image = image.squeeze(0)\n",
    "            batch_size = 16\n",
    "            whole_size = image.size(0)\n",
    "            if whole_size % batch_size == 0:\n",
    "                n_iter = whole_size // batch_size\n",
    "            else:\n",
    "                n_iter = whole_size // batch_size + 1\n",
    "                \n",
    "            all_events = set()\n",
    "            for batch_i in range(n_iter):\n",
    "                batch = image[batch_i * batch_size : (batch_i + 1) * batch_size]\n",
    "                \n",
    "                if batch.ndim == 3:\n",
    "                    batch = batch.unsqueeze(0)\n",
    "                    \n",
    "                batch = batch.to(device)\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(batch)\n",
    "                    proba = prediction[\"multilabel_prob\"].detach().cpu().numpy()\n",
    "                    \n",
    "                    events = proba >= threshold\n",
    "                    for i in range(len(events)):\n",
    "                        event = events[i, :]\n",
    "                        labels = np.argwhere(event).reshape(-1).tolist()\n",
    "                        \n",
    "                        for label in labels:\n",
    "                            all_events.add(label)\n",
    "            labels = list(all_events)\n",
    "        if len(labels) == 0:\n",
    "            prediction_dict[row_id] = \"nocall\"\n",
    "            \n",
    "        else:\n",
    "            label_str_list = list(map(lambda x: INV_BIRD_CODE[x], labels))\n",
    "            label_string = \" \".join(label_str_list)\n",
    "            prediction_dict[row_id] = label_string\n",
    "            \n",
    "    return prediction_dict\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_df : pd.DataFrame, test_audio: Path, model_config: dict, mel_params: dict, threshold=0.5):\n",
    "    model = get_model(model_config)\n",
    "    unique_audio_id = test[\"audio_id\"].unique()\n",
    "    check_audio = os.listdir(\"../input/birdcall-check/test_audio\")\n",
    "#     warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    predictions_dfs = []\n",
    "    for audio_id in check_audio:\n",
    "        \n",
    "        clip, _ = librosa.load(test_audio / (audio_id ),\n",
    "                                   sr=SR,\n",
    "                                   mono=True,\n",
    "                                   res_type=\"kaiser_fast\")\n",
    "        test_df_for_audio_id = test.query(f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n",
    "        \n",
    "        with timer(f\"Prediction on {audio_id}\", logger):\n",
    "            prediction_dict = prediction_for_clip(test, clip=clip, model=model, mel_params=mel_params, threshold=threshold)\n",
    "            \n",
    "            row_id = list(prediction_dict.keys())\n",
    "            birds = list(prediction_dict.values())\n",
    "            prediction_df = pd.DataFrame({\"row_id\": row_id, \"birds\": birds})\n",
    "            predictions_dfs.append(prediction_df)\n",
    "            \n",
    "        prediction_df = pd.concat(predictions_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "        return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = prediction(test_df=test,\n",
    "                        test_audio=test_audio,\n",
    "                        model_config=model_config,\n",
    "                        mel_params=melspec_params,\n",
    "                        threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
